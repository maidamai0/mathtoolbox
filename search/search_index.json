{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"mathtoolbox Mathematical tools (interpolation, dimensionality reduction, optimization, etc.) written in C++11 and Eigen . Algorithms Scattered Data Interpolation and Function Approximation rbf-interpolation : Radial basis function (RBF) interpolation gaussian-process-regression : Gaussian process regression (GPR) Dimensionality Reduction and Low-Dimensional Embedding classical-mds : Classical multi-dimensional scaling (MDS) som : Self-organizing map (SOM) Numerical Optimization backtracking-line-search : Backtracking line search bayesian-optimization : Bayesian optimization bfgs : BFGS method gradient-descent : Gradient descent method l-bfgs : Limited-memory BFGS method strong-wolfe-conditions-line-search : Strong Wolfe conditions line search Linear Algebra log-determinant : Log-determinant matrix-inversion : Matrix inversion techniques Utilities acquisition-functions : Acquisition functions constants : Constants data-normalization : Data normalization kernel-functions : Kernel functions probability-distributions : Probability distributions Dependencies Main Library Eigen http://eigen.tuxfamily.org/ ( brew install eigen / sudo apt install libeigen3-dev ) Python Bindings pybind11 https://github.com/pybind/pybind11 (included as gitsubmodule) Examples optimization-test-function https://github.com/yuki-koyama/optimization-test-functions (included as gitsubmodule) Use as a C++ Library mathtoolbox uses CMake https://cmake.org/ for building source codes. This library can be built, for example, by git clone https://github.com/yuki-koyama/mathtoolbox.git --recursive cd mathtoolbox mkdir build cd build cmake ../ make and optionally it can be installed to the system by make install When the CMake parameter MATHTOOLBOX_BUILD_EXAMPLES is set ON , the example applications are also built. (The default setting is OFF .) This is done by cmake ../ -DMATHTOOLBOX_BUILD_EXAMPLES=ON make When the CMake parameter MATHTOOLBOX_PYTHON_BINDINGS is set ON , the example applications are also built. (The default setting is OFF .) This is done by cmake ../ -DMATHTOOLBOX_PYTHON_BINDINGS=ON make Prerequisites macOS: brew install eigen Ubuntu: sudo apt install libeigen3-dev Use as a Python Library pymathtoolbox is a (sub)set of Python bindings of mathtoolbox. Tested on Python 3.6 , 3.7 , and 3.8 . It can be installed via PyPI: pip install git+https://github.com/yuki-koyama/mathtoolbox Prerequisites macOS brew install cmake eigen Ubuntu 16.04/18.04 sudo apt install cmake libeigen3-dev Examples See python-examples . Gallery Bayesian optimization ( bayesian-optimization ) solves a one-dimensional optimization problem using only a small number of function-evaluation queries. Classical multi-dimensional scaling ( classical-mds ) is applied to pixel RGB values of a target image to embed them into a two-dimensional space. Self-organizing map ( som ) is also applicable to pixel RGB values of a target image to learn a two-dimensional color manifold. Projects Using mathtoolbox SelPh https://github.com/yuki-koyama/selph (for classical-mds ) Sequential Line Search https://github.com/yuki-koyama/sequential-line-search (for acquisition-functions , kernel-functions , log-determinant , and probability-distributions ) Contributing Bug reports, suggestions, feature requests, and PRs are highly welcomed. Licensing The MIT License.","title":"Introduction"},{"location":"#mathtoolbox","text":"Mathematical tools (interpolation, dimensionality reduction, optimization, etc.) written in C++11 and Eigen .","title":"mathtoolbox"},{"location":"#algorithms","text":"","title":"Algorithms"},{"location":"#scattered-data-interpolation-and-function-approximation","text":"rbf-interpolation : Radial basis function (RBF) interpolation gaussian-process-regression : Gaussian process regression (GPR)","title":"Scattered Data Interpolation and Function Approximation"},{"location":"#dimensionality-reduction-and-low-dimensional-embedding","text":"classical-mds : Classical multi-dimensional scaling (MDS) som : Self-organizing map (SOM)","title":"Dimensionality Reduction and Low-Dimensional Embedding"},{"location":"#numerical-optimization","text":"backtracking-line-search : Backtracking line search bayesian-optimization : Bayesian optimization bfgs : BFGS method gradient-descent : Gradient descent method l-bfgs : Limited-memory BFGS method strong-wolfe-conditions-line-search : Strong Wolfe conditions line search","title":"Numerical Optimization"},{"location":"#linear-algebra","text":"log-determinant : Log-determinant matrix-inversion : Matrix inversion techniques","title":"Linear Algebra"},{"location":"#utilities","text":"acquisition-functions : Acquisition functions constants : Constants data-normalization : Data normalization kernel-functions : Kernel functions probability-distributions : Probability distributions","title":"Utilities"},{"location":"#dependencies","text":"","title":"Dependencies"},{"location":"#main-library","text":"Eigen http://eigen.tuxfamily.org/ ( brew install eigen / sudo apt install libeigen3-dev )","title":"Main Library"},{"location":"#python-bindings","text":"pybind11 https://github.com/pybind/pybind11 (included as gitsubmodule)","title":"Python Bindings"},{"location":"#examples","text":"optimization-test-function https://github.com/yuki-koyama/optimization-test-functions (included as gitsubmodule)","title":"Examples"},{"location":"#use-as-a-c-library","text":"mathtoolbox uses CMake https://cmake.org/ for building source codes. This library can be built, for example, by git clone https://github.com/yuki-koyama/mathtoolbox.git --recursive cd mathtoolbox mkdir build cd build cmake ../ make and optionally it can be installed to the system by make install When the CMake parameter MATHTOOLBOX_BUILD_EXAMPLES is set ON , the example applications are also built. (The default setting is OFF .) This is done by cmake ../ -DMATHTOOLBOX_BUILD_EXAMPLES=ON make When the CMake parameter MATHTOOLBOX_PYTHON_BINDINGS is set ON , the example applications are also built. (The default setting is OFF .) This is done by cmake ../ -DMATHTOOLBOX_PYTHON_BINDINGS=ON make","title":"Use as a C++ Library"},{"location":"#prerequisites","text":"macOS: brew install eigen Ubuntu: sudo apt install libeigen3-dev","title":"Prerequisites"},{"location":"#use-as-a-python-library","text":"pymathtoolbox is a (sub)set of Python bindings of mathtoolbox. Tested on Python 3.6 , 3.7 , and 3.8 . It can be installed via PyPI: pip install git+https://github.com/yuki-koyama/mathtoolbox","title":"Use as a Python Library"},{"location":"#prerequisites_1","text":"macOS brew install cmake eigen Ubuntu 16.04/18.04 sudo apt install cmake libeigen3-dev","title":"Prerequisites"},{"location":"#examples_1","text":"See python-examples .","title":"Examples"},{"location":"#gallery","text":"Bayesian optimization ( bayesian-optimization ) solves a one-dimensional optimization problem using only a small number of function-evaluation queries. Classical multi-dimensional scaling ( classical-mds ) is applied to pixel RGB values of a target image to embed them into a two-dimensional space. Self-organizing map ( som ) is also applicable to pixel RGB values of a target image to learn a two-dimensional color manifold.","title":"Gallery"},{"location":"#projects-using-mathtoolbox","text":"SelPh https://github.com/yuki-koyama/selph (for classical-mds ) Sequential Line Search https://github.com/yuki-koyama/sequential-line-search (for acquisition-functions , kernel-functions , log-determinant , and probability-distributions )","title":"Projects Using mathtoolbox"},{"location":"#contributing","text":"Bug reports, suggestions, feature requests, and PRs are highly welcomed.","title":"Contributing"},{"location":"#licensing","text":"The MIT License.","title":"Licensing"},{"location":"acquisition-functions/","text":"acquisition-functions Acquisition functions for Bayesian optimization Header #include <mathtoolbox/acquisition-functions.hpp> Overview The following acquisition functions (and their derivatives with respect to the data point) are supported: Expected improvement (EI) Gaussian process upper confidence bound (GP-UCB) Useful Resources Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. 2012. Practical Bayesian optimization of machine learning algorithms. In Proc. NIPS '12, pp.2951--2959. Yuki Koyama, Issei Sato, Daisuke Sakamoto, and Takeo Igarashi. 2017. Sequential line search for efficient visual design optimization by crowds. ACM Trans. Graph. 36, 4, pp.48:1--48:11 (2017). DOI: https://doi.org/10.1145/3072959.3073598 Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. 2010. Gaussian process optimization in the bandit setting: no regret and experimental design. In Proc. ICML '10, pp.1015--1022.","title":"acquisition-functions"},{"location":"acquisition-functions/#acquisition-functions","text":"Acquisition functions for Bayesian optimization","title":"acquisition-functions"},{"location":"acquisition-functions/#header","text":"#include <mathtoolbox/acquisition-functions.hpp>","title":"Header"},{"location":"acquisition-functions/#overview","text":"The following acquisition functions (and their derivatives with respect to the data point) are supported: Expected improvement (EI) Gaussian process upper confidence bound (GP-UCB)","title":"Overview"},{"location":"acquisition-functions/#useful-resources","text":"Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. 2012. Practical Bayesian optimization of machine learning algorithms. In Proc. NIPS '12, pp.2951--2959. Yuki Koyama, Issei Sato, Daisuke Sakamoto, and Takeo Igarashi. 2017. Sequential line search for efficient visual design optimization by crowds. ACM Trans. Graph. 36, 4, pp.48:1--48:11 (2017). DOI: https://doi.org/10.1145/3072959.3073598 Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. 2010. Gaussian process optimization in the bandit setting: no regret and experimental design. In Proc. ICML '10, pp.1015--1022.","title":"Useful Resources"},{"location":"backtracking-line-search/","text":"backtracking-line-search A line search method for finding a step size that satisfies the Armijo (i.e., sufficient decrease) condition based on a simple backtracking procedure. Header #include <mathtoolbox/backtracking-line-search.hpp> Math and Algorithm We follow Nocedal and Wright (2006) (Chapter 3, specifically Algorithm 3.1). Useful Resources Jorge Nocedal and Stephen J. Wright. 2006. Numerical optimization (2nd ed.). Springer. DOI: https://doi.org/10.1007/978-0-387-40065-5","title":"backtracking-line-search"},{"location":"backtracking-line-search/#backtracking-line-search","text":"A line search method for finding a step size that satisfies the Armijo (i.e., sufficient decrease) condition based on a simple backtracking procedure.","title":"backtracking-line-search"},{"location":"backtracking-line-search/#header","text":"#include <mathtoolbox/backtracking-line-search.hpp>","title":"Header"},{"location":"backtracking-line-search/#math-and-algorithm","text":"We follow Nocedal and Wright (2006) (Chapter 3, specifically Algorithm 3.1).","title":"Math and Algorithm"},{"location":"backtracking-line-search/#useful-resources","text":"Jorge Nocedal and Stephen J. Wright. 2006. Numerical optimization (2nd ed.). Springer. DOI: https://doi.org/10.1007/978-0-387-40065-5","title":"Useful Resources"},{"location":"bayesian-optimization/","text":"bayesian-optimization Bayesian optimization (BO) is a black-box global optimization algorithm. During the iterative process, this algorithm determines the next sampling point based on Bayesian inference of the latent function. BO is likely to find a reasonable solution with fewer function evaluations than other optimization algorithms. Thus, it is suitable for problems with expensive-to-evaluate objective functions. Header #include <mathtoolbox/bayesian-optimization.hpp> Internal Dependencies acquisition-functions gradient-descent gaussian-process-regression Math and Algorithm Problem Formulation This implementation is for solving maximization problems in the following form: \\mathbf{x}^{*} = \\mathop{\\rm arg~max}\\limits_{\\mathbf{x} \\in \\mathcal{X}} \\: f(\\mathbf{x}), where \\mathbf{x} \\in \\mathbb{R}^{n} represents search variables, \\mathcal{X} \\subset \\mathbb{R}^{n} is the search space, and f : \\mathcal{X} \\rightarrow \\mathbb{R} is the objective function to be maximized. Currently, this implementation assumes that the search space \\mathcal{X} is a hyperrectangle represented as \\mathcal{X} = \\{ \\mathbf{x} \\mid l_{i} \\leq x_{i} \\leq u_{i}, i = 1, \\ldots, n \\}. Surrogate Function Representation This implementation assumes the Gaussian process prior. That is, it performs Gaussian process regression using the observed data. (TODO) Acquisition Functions Currently, it supports the expected improvement (EI) function only. (TODO) Examples Optimizing a One-Dimensional Function Performance The following plot shows a result of optimizing a simple objective function: \\max_{\\mathbf{x} \\in [-1, 1]^{5}} \\{ - \\| \\mathbf{x} \\|^{2} \\} 50 times with random initial solutions. As a baseline, it also shows a result by the same setting except for using random uniform sampling instead of BO. // Define the target problem const int num_dims = 5; const auto f = [](const Eigen::VectorXd& x) { return - x.squaredNorm(); }; const auto lower_bound = Eigen::VectorXd::Constant(num_dims, -1.0); const auto upper_bound = Eigen::VectorXd::Constant(num_dims, +1.0); // Instantiate an optimizer mathtoolbox::BayesianOptimizer optimizer{f, lower_bound, upper_bound}; // Perform optimization iteration for (int i = 0; i < 50; ++i) { optimizer.Step(); } // Retrieve the found solution const Eigen::VectorXd x_star = optimizer.GetCurrentSolution(); Useful Resources Yuki Koyama, Issei Sato, Daisuke Sakamoto, and Takeo Igarashi. 2017. Sequential line search for efficient visual design optimization by crowds. ACM Trans. Graph. 36, 4, pp.48:1--48:11 (2017). DOI: https://doi.org/10.1145/3072959.3073598 Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P. Adams, and Nando de Freitas. 2016. Taking the Human Out of the Loop: A Review of Bayesian Optimization. Proc. IEEE 104, 1, pp.148--175 (2016). DOI: https://doi.org/10.1109/JPROC.2015.2494218 (TODO)","title":"bayesian-optimization"},{"location":"bayesian-optimization/#bayesian-optimization","text":"Bayesian optimization (BO) is a black-box global optimization algorithm. During the iterative process, this algorithm determines the next sampling point based on Bayesian inference of the latent function. BO is likely to find a reasonable solution with fewer function evaluations than other optimization algorithms. Thus, it is suitable for problems with expensive-to-evaluate objective functions.","title":"bayesian-optimization"},{"location":"bayesian-optimization/#header","text":"#include <mathtoolbox/bayesian-optimization.hpp>","title":"Header"},{"location":"bayesian-optimization/#internal-dependencies","text":"acquisition-functions gradient-descent gaussian-process-regression","title":"Internal Dependencies"},{"location":"bayesian-optimization/#math-and-algorithm","text":"","title":"Math and Algorithm"},{"location":"bayesian-optimization/#problem-formulation","text":"This implementation is for solving maximization problems in the following form: \\mathbf{x}^{*} = \\mathop{\\rm arg~max}\\limits_{\\mathbf{x} \\in \\mathcal{X}} \\: f(\\mathbf{x}), where \\mathbf{x} \\in \\mathbb{R}^{n} represents search variables, \\mathcal{X} \\subset \\mathbb{R}^{n} is the search space, and f : \\mathcal{X} \\rightarrow \\mathbb{R} is the objective function to be maximized. Currently, this implementation assumes that the search space \\mathcal{X} is a hyperrectangle represented as \\mathcal{X} = \\{ \\mathbf{x} \\mid l_{i} \\leq x_{i} \\leq u_{i}, i = 1, \\ldots, n \\}.","title":"Problem Formulation"},{"location":"bayesian-optimization/#surrogate-function-representation","text":"This implementation assumes the Gaussian process prior. That is, it performs Gaussian process regression using the observed data. (TODO)","title":"Surrogate Function Representation"},{"location":"bayesian-optimization/#acquisition-functions","text":"Currently, it supports the expected improvement (EI) function only. (TODO)","title":"Acquisition Functions"},{"location":"bayesian-optimization/#examples","text":"","title":"Examples"},{"location":"bayesian-optimization/#optimizing-a-one-dimensional-function","text":"","title":"Optimizing a One-Dimensional Function"},{"location":"bayesian-optimization/#performance","text":"The following plot shows a result of optimizing a simple objective function: \\max_{\\mathbf{x} \\in [-1, 1]^{5}} \\{ - \\| \\mathbf{x} \\|^{2} \\} 50 times with random initial solutions. As a baseline, it also shows a result by the same setting except for using random uniform sampling instead of BO. // Define the target problem const int num_dims = 5; const auto f = [](const Eigen::VectorXd& x) { return - x.squaredNorm(); }; const auto lower_bound = Eigen::VectorXd::Constant(num_dims, -1.0); const auto upper_bound = Eigen::VectorXd::Constant(num_dims, +1.0); // Instantiate an optimizer mathtoolbox::BayesianOptimizer optimizer{f, lower_bound, upper_bound}; // Perform optimization iteration for (int i = 0; i < 50; ++i) { optimizer.Step(); } // Retrieve the found solution const Eigen::VectorXd x_star = optimizer.GetCurrentSolution();","title":"Performance"},{"location":"bayesian-optimization/#useful-resources","text":"Yuki Koyama, Issei Sato, Daisuke Sakamoto, and Takeo Igarashi. 2017. Sequential line search for efficient visual design optimization by crowds. ACM Trans. Graph. 36, 4, pp.48:1--48:11 (2017). DOI: https://doi.org/10.1145/3072959.3073598 Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P. Adams, and Nando de Freitas. 2016. Taking the Human Out of the Loop: A Review of Bayesian Optimization. Proc. IEEE 104, 1, pp.148--175 (2016). DOI: https://doi.org/10.1109/JPROC.2015.2494218 (TODO)","title":"Useful Resources"},{"location":"bfgs/","text":"bfgs The BFGS method (BFGS) is a numerical optimization algorithm that is one of the most popular choices among quasi-Newton methods. Header #include <mathtoolbox/bfgs.hpp> Internal Dependencies strong-wolfe-conditions-line-search Math and Algorithm We follow Nocedal and Wright (2006) (Chapter 6). Inverse Hessian Initialization This implementation adopts the strategy described in Equation 6.20: \\mathbf{H}_0 \\leftarrow \\frac{\\mathbf{y}_k^T \\mathbf{s}_k}{\\mathbf{y}_k^T \\mathbf{y}_k} \\mathbf{I}. See the book for details. Line Search This implementation uses strong-wolfe-conditions-line-search to find an appropriate step size. Useful Resources Jorge Nocedal and Stephen J. Wright. 2006. Numerical optimization (2nd ed.). Springer. DOI: https://doi.org/10.1007/978-0-387-40065-5","title":"bfgs"},{"location":"bfgs/#bfgs","text":"The BFGS method (BFGS) is a numerical optimization algorithm that is one of the most popular choices among quasi-Newton methods.","title":"bfgs"},{"location":"bfgs/#header","text":"#include <mathtoolbox/bfgs.hpp>","title":"Header"},{"location":"bfgs/#internal-dependencies","text":"strong-wolfe-conditions-line-search","title":"Internal Dependencies"},{"location":"bfgs/#math-and-algorithm","text":"We follow Nocedal and Wright (2006) (Chapter 6).","title":"Math and Algorithm"},{"location":"bfgs/#inverse-hessian-initialization","text":"This implementation adopts the strategy described in Equation 6.20: \\mathbf{H}_0 \\leftarrow \\frac{\\mathbf{y}_k^T \\mathbf{s}_k}{\\mathbf{y}_k^T \\mathbf{y}_k} \\mathbf{I}. See the book for details.","title":"Inverse Hessian Initialization"},{"location":"bfgs/#line-search","text":"This implementation uses strong-wolfe-conditions-line-search to find an appropriate step size.","title":"Line Search"},{"location":"bfgs/#useful-resources","text":"Jorge Nocedal and Stephen J. Wright. 2006. Numerical optimization (2nd ed.). Springer. DOI: https://doi.org/10.1007/978-0-387-40065-5","title":"Useful Resources"},{"location":"classical-mds/","text":"classical-mds Classical multi-dimensional scaling (MDS) for dimensionality reduction and low-dimensional embedding. This is also useful for visualizing the similarities of individual items in a 2-dimensional scattered plot. Header #include <mathtoolbox/classical-mds.hpp> Math Overview Given a distance (or dissimilarity) matrix of n elements \\mathbf{D} \\in \\mathbb{R}^{n \\times n} and a target dimensionality m , this technique calculates a set of m -dimensional coordinates for them: \\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1 & \\cdots & \\mathbf{x}_n \\end{bmatrix} \\in \\mathbb{R}^{m \\times n}. If the elements are originally defined in an m' -dimensional space ( m < m' ) and Euclidian distance is used for calculating the distance matrix, then this is considered dimensionality reduction (or low-dimensional embedding). Algorithm First, calculate the kernel matrix: \\mathbf{K} = - \\frac{1}{2} \\mathbf{H} \\mathbf{D}^{(2)} \\mathbf{H} \\in \\mathbb{R}^{n \\times n}, where \\mathbf{H} is called the centering matrix and defined as \\mathbf{H} = \\mathbf{I} - \\frac{1}{n} \\mathbf{1}^T \\mathbf{1} \\in \\mathbb{R}^{n \\times n}, and \\mathbf{D}^{(2)} is the squared distance matrix. Then, apply eigenvalue decomposition to \\mathbf{K} : \\mathbf{K} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^T. Finally, pick up the m -largest eigenvalues \\mathbf{\\Lambda}_m and corresponding eigenvectors \\mathbf{V}_m , and calculate \\mathbf{X} by \\mathbf{X} = \\mathbf{V}_m \\mathbf{\\Lambda}_m^\\frac{1}{2}. Usage This technique can be calculated by the following function: Eigen::MatrixXd ComputeClassicalMds(const Eigen::MatrixXd& D, const unsigned target_dim); where target_dim is the target dimensionality for embedding. Example The following is an example of applying the algorithm to the pixel RGB values of a target image and embedding them into a two-dimensional space. Useful Resources Josh Wills, Sameer Agarwal, David Kriegman, and Serge Belongie. 2009. Toward a perceptual space for gloss. ACM Trans. Graph. 28, 4, Article 103 (September 2009), 15 pages. DOI: https://doi.org/10.1145/1559755.1559760","title":"classical-mds"},{"location":"classical-mds/#classical-mds","text":"Classical multi-dimensional scaling (MDS) for dimensionality reduction and low-dimensional embedding. This is also useful for visualizing the similarities of individual items in a 2-dimensional scattered plot.","title":"classical-mds"},{"location":"classical-mds/#header","text":"#include <mathtoolbox/classical-mds.hpp>","title":"Header"},{"location":"classical-mds/#math","text":"","title":"Math"},{"location":"classical-mds/#overview","text":"Given a distance (or dissimilarity) matrix of n elements \\mathbf{D} \\in \\mathbb{R}^{n \\times n} and a target dimensionality m , this technique calculates a set of m -dimensional coordinates for them: \\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1 & \\cdots & \\mathbf{x}_n \\end{bmatrix} \\in \\mathbb{R}^{m \\times n}. If the elements are originally defined in an m' -dimensional space ( m < m' ) and Euclidian distance is used for calculating the distance matrix, then this is considered dimensionality reduction (or low-dimensional embedding).","title":"Overview"},{"location":"classical-mds/#algorithm","text":"First, calculate the kernel matrix: \\mathbf{K} = - \\frac{1}{2} \\mathbf{H} \\mathbf{D}^{(2)} \\mathbf{H} \\in \\mathbb{R}^{n \\times n}, where \\mathbf{H} is called the centering matrix and defined as \\mathbf{H} = \\mathbf{I} - \\frac{1}{n} \\mathbf{1}^T \\mathbf{1} \\in \\mathbb{R}^{n \\times n}, and \\mathbf{D}^{(2)} is the squared distance matrix. Then, apply eigenvalue decomposition to \\mathbf{K} : \\mathbf{K} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^T. Finally, pick up the m -largest eigenvalues \\mathbf{\\Lambda}_m and corresponding eigenvectors \\mathbf{V}_m , and calculate \\mathbf{X} by \\mathbf{X} = \\mathbf{V}_m \\mathbf{\\Lambda}_m^\\frac{1}{2}.","title":"Algorithm"},{"location":"classical-mds/#usage","text":"This technique can be calculated by the following function: Eigen::MatrixXd ComputeClassicalMds(const Eigen::MatrixXd& D, const unsigned target_dim); where target_dim is the target dimensionality for embedding.","title":"Usage"},{"location":"classical-mds/#example","text":"The following is an example of applying the algorithm to the pixel RGB values of a target image and embedding them into a two-dimensional space.","title":"Example"},{"location":"classical-mds/#useful-resources","text":"Josh Wills, Sameer Agarwal, David Kriegman, and Serge Belongie. 2009. Toward a perceptual space for gloss. ACM Trans. Graph. 28, 4, Article 103 (September 2009), 15 pages. DOI: https://doi.org/10.1145/1559755.1559760","title":"Useful Resources"},{"location":"constants/","text":"constants Constants used for various mathematical contexts. Header #include <mathtoolbox/constants.hpp> Constants are defined in the namespace mathtoolbox::constants . Pi pi = 3.14159265358979323846264338327950288; Note that those who use C++20 or later should use <numbers> instead.","title":"constants"},{"location":"constants/#constants","text":"Constants used for various mathematical contexts.","title":"constants"},{"location":"constants/#header","text":"#include <mathtoolbox/constants.hpp> Constants are defined in the namespace mathtoolbox::constants .","title":"Header"},{"location":"constants/#pi","text":"pi = 3.14159265358979323846264338327950288; Note that those who use C++20 or later should use <numbers> instead.","title":"Pi"},{"location":"data-normalization/","text":"data-normalization Data normalization for preprocessing. Header #include <mathtoolbox/data-normalization.hpp> Math This implementation is based on simple standardization for each dimension d : X_{i}^{(d)} \\leftarrow \\frac{X_{i}^{(d)} - \\text{E}[X^{(d)}]}{\\sigma(X^{(d)})}.","title":"data-normalization"},{"location":"data-normalization/#data-normalization","text":"Data normalization for preprocessing.","title":"data-normalization"},{"location":"data-normalization/#header","text":"#include <mathtoolbox/data-normalization.hpp>","title":"Header"},{"location":"data-normalization/#math","text":"This implementation is based on simple standardization for each dimension d : X_{i}^{(d)} \\leftarrow \\frac{X_{i}^{(d)} - \\text{E}[X^{(d)}]}{\\sigma(X^{(d)})}.","title":"Math"},{"location":"gaussian-process-regression/","text":"gaussian-process-regression Gaussian process regression (GPR) for scattered data interpolation and function approximation. Header #include <mathtoolbox/gaussian-process-regression.hpp> Internal Dependencies constants kernel-functions l-bfgs log-determinant Overview Input The input consists of a set of N scattered data points: \\{ (\\mathbf{x}_i, y_i) \\}_{i = 1}^{N}, where \\mathbf{x}_i \\in \\mathbb{R}^D is the i -th data point location in a D -dimensional space and y_i \\in \\mathbb{R} is its associated value. This input data is also denoted as \\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_{1} & \\cdots & \\mathbf{x}_{N} \\end{bmatrix} \\in \\mathbb{R}^{D \\times N} and \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_N \\end{bmatrix} \\in \\mathbb{R}^{N}. Here, each observed values y_i is assumed to be a noisy version of the corresponding latent function value f_i = f(\\mathbf{x}_i) . More specifically, y_i = f_i + \\delta, where \\delta \\sim \\mathcal{N}(0, \\sigma_n^{2}). \\sigma_n^{2} (the noise variance) is considered as one of the hyperparamters of this model. Output Given the data and the Gaussian process assumption, GPR can calculate the most likely value f_{*} and its variance \\text{Var}(f_{*}) for an arbitrary location \\mathbf{x}_{*} . The variance indicates how uncertain the estimation is. For example, when this value is large, the estimated value may not be very trustful (this often occurs in regions with less data points). As the predicted value follows a Gaussian, its 95%-confidence interval can be obtained by [ f_{*} - 1.96 \\sqrt{\\text{Var}(f_{*})}, f_{*} + 1.96 \\sqrt{\\text{Var}(f_{*})} ] . Math Covariance Function The automatic relevance determination (ARD) Matern 5/2 kernel is the default choice: k(\\mathbf{x}_p, \\mathbf{x}_q ; \\sigma_f^{2}, \\boldsymbol{\\ell}) = \\sigma_f^{2} \\left( 1 + \\sqrt{5 r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell})} + \\frac{5}{3} r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell}) \\right) \\exp \\left\\{ - \\sqrt{5 r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell})} \\right\\}, where r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell}) = (\\mathbf{x}_p - \\mathbf{x}_q)^{T} \\text{diag}(\\boldsymbol{\\ell})^{-2} (\\mathbf{x}_p - \\mathbf{x}_q) and \\sigma_f^{2} (the signal variance) and \\boldsymbol{\\ell} (the characteristic length-scales) are its hyperparameters. That is, \\boldsymbol{\\theta}_\\text{kernel} = \\begin{bmatrix} \\sigma_f^{2} \\\\ \\boldsymbol{\\ell} \\end{bmatrix} \\in \\mathbb{R}^{D + 1}. Mean Function A constant-value function is used: m(\\mathbf{x}) = 0. Data Normalization Optionally, this implementation offers an automatic data normalization functionality. If this is enabled, it applies the following normalization: y_{i} \\leftarrow s \\cdot \\frac{y_{i} - \\mu}{\\sigma} \\:\\: \\text{for} \\:\\: i = 1, \\ldots, N, where s is an empirically selected scaling coefficient. This normalization is sometimes useful for the regression to be more robust for various datasets without drastically changing hyperparameters. Selecting Hyperparameters There are two options for setting hyperparameters: Set manually Determined by the maximum likelihood estimation Maximum Likelihood Estimation Let \\boldsymbol{\\theta} be a concatenation of hyperparameters; that is, \\boldsymbol{\\theta} = \\begin{bmatrix} \\boldsymbol{\\theta}_\\text{kernel} \\\\ \\sigma_{n}^{2} \\end{bmatrix} \\in \\mathbb{R}^{D + 2}. In this approach, these hyperparameters are determined by solving the following numerical optimization problem: \\boldsymbol{\\theta}^\\text{ML} = \\mathop{\\rm arg~max}\\limits_{\\boldsymbol{\\theta}} p(\\mathbf{y} \\mid \\mathbf{X}, \\boldsymbol{\\theta}). In this implementation, this maximization problem is solved by the L-BFGS method (a gradient-based local optimization algorithm). An initial solution for this maximization needs to be specified. Usage Instantiation and Data Specification A GPR object is instantiated with data specification in its constructor: GaussianProcessRegressor(const Eigen::MatrixXd& X, const Eigen::VectorXd& y, const KernelType kernel_type = KernelType::ArdMatern52, const bool use_data_normalization = true); Hyperparameter Selection Hyperparameters are set by either void SetHyperparams(const Eigen::VectorXd& kernel_hyperparams, const double noise_hyperparam); or void PerformMaximumLikelihood(const Eigen::VectorXd& kernel_hyperparams_initial, const double noise_hyperparam_initial); Prediction Once a GPR object is instantiated and its hyperparameters are set, it is ready for prediction. For an unknown location \\mathbf{x} , the GPR object predicts the most likely value f by the following method: double PredictMean(const Eigen::VectorXd& x) const; It also predicts the standard deviation \\sqrt{\\text{Var}(f)} by the following method: double PredictStdev(const Eigen::VectorXd& x) const; Useful Resources Mark Ebden. 2015. Gaussian Processes: A Quick Introduction. arXiv:1505.02965 . Carl Edward Rasmussen and Christopher K. I. Williams. 2006. Gaussian Processes for Machine Learning. The MIT Press. Online version: http://www.gaussianprocess.org/gpml/","title":"gaussian-process-regression"},{"location":"gaussian-process-regression/#gaussian-process-regression","text":"Gaussian process regression (GPR) for scattered data interpolation and function approximation.","title":"gaussian-process-regression"},{"location":"gaussian-process-regression/#header","text":"#include <mathtoolbox/gaussian-process-regression.hpp>","title":"Header"},{"location":"gaussian-process-regression/#internal-dependencies","text":"constants kernel-functions l-bfgs log-determinant","title":"Internal Dependencies"},{"location":"gaussian-process-regression/#overview","text":"","title":"Overview"},{"location":"gaussian-process-regression/#input","text":"The input consists of a set of N scattered data points: \\{ (\\mathbf{x}_i, y_i) \\}_{i = 1}^{N}, where \\mathbf{x}_i \\in \\mathbb{R}^D is the i -th data point location in a D -dimensional space and y_i \\in \\mathbb{R} is its associated value. This input data is also denoted as \\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_{1} & \\cdots & \\mathbf{x}_{N} \\end{bmatrix} \\in \\mathbb{R}^{D \\times N} and \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_N \\end{bmatrix} \\in \\mathbb{R}^{N}. Here, each observed values y_i is assumed to be a noisy version of the corresponding latent function value f_i = f(\\mathbf{x}_i) . More specifically, y_i = f_i + \\delta, where \\delta \\sim \\mathcal{N}(0, \\sigma_n^{2}). \\sigma_n^{2} (the noise variance) is considered as one of the hyperparamters of this model.","title":"Input"},{"location":"gaussian-process-regression/#output","text":"Given the data and the Gaussian process assumption, GPR can calculate the most likely value f_{*} and its variance \\text{Var}(f_{*}) for an arbitrary location \\mathbf{x}_{*} . The variance indicates how uncertain the estimation is. For example, when this value is large, the estimated value may not be very trustful (this often occurs in regions with less data points). As the predicted value follows a Gaussian, its 95%-confidence interval can be obtained by [ f_{*} - 1.96 \\sqrt{\\text{Var}(f_{*})}, f_{*} + 1.96 \\sqrt{\\text{Var}(f_{*})} ] .","title":"Output"},{"location":"gaussian-process-regression/#math","text":"","title":"Math"},{"location":"gaussian-process-regression/#covariance-function","text":"The automatic relevance determination (ARD) Matern 5/2 kernel is the default choice: k(\\mathbf{x}_p, \\mathbf{x}_q ; \\sigma_f^{2}, \\boldsymbol{\\ell}) = \\sigma_f^{2} \\left( 1 + \\sqrt{5 r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell})} + \\frac{5}{3} r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell}) \\right) \\exp \\left\\{ - \\sqrt{5 r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell})} \\right\\}, where r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell}) = (\\mathbf{x}_p - \\mathbf{x}_q)^{T} \\text{diag}(\\boldsymbol{\\ell})^{-2} (\\mathbf{x}_p - \\mathbf{x}_q) and \\sigma_f^{2} (the signal variance) and \\boldsymbol{\\ell} (the characteristic length-scales) are its hyperparameters. That is, \\boldsymbol{\\theta}_\\text{kernel} = \\begin{bmatrix} \\sigma_f^{2} \\\\ \\boldsymbol{\\ell} \\end{bmatrix} \\in \\mathbb{R}^{D + 1}.","title":"Covariance Function"},{"location":"gaussian-process-regression/#mean-function","text":"A constant-value function is used: m(\\mathbf{x}) = 0.","title":"Mean Function"},{"location":"gaussian-process-regression/#data-normalization","text":"Optionally, this implementation offers an automatic data normalization functionality. If this is enabled, it applies the following normalization: y_{i} \\leftarrow s \\cdot \\frac{y_{i} - \\mu}{\\sigma} \\:\\: \\text{for} \\:\\: i = 1, \\ldots, N, where s is an empirically selected scaling coefficient. This normalization is sometimes useful for the regression to be more robust for various datasets without drastically changing hyperparameters.","title":"Data Normalization"},{"location":"gaussian-process-regression/#selecting-hyperparameters","text":"There are two options for setting hyperparameters: Set manually Determined by the maximum likelihood estimation","title":"Selecting Hyperparameters"},{"location":"gaussian-process-regression/#maximum-likelihood-estimation","text":"Let \\boldsymbol{\\theta} be a concatenation of hyperparameters; that is, \\boldsymbol{\\theta} = \\begin{bmatrix} \\boldsymbol{\\theta}_\\text{kernel} \\\\ \\sigma_{n}^{2} \\end{bmatrix} \\in \\mathbb{R}^{D + 2}. In this approach, these hyperparameters are determined by solving the following numerical optimization problem: \\boldsymbol{\\theta}^\\text{ML} = \\mathop{\\rm arg~max}\\limits_{\\boldsymbol{\\theta}} p(\\mathbf{y} \\mid \\mathbf{X}, \\boldsymbol{\\theta}). In this implementation, this maximization problem is solved by the L-BFGS method (a gradient-based local optimization algorithm). An initial solution for this maximization needs to be specified.","title":"Maximum Likelihood Estimation"},{"location":"gaussian-process-regression/#usage","text":"","title":"Usage"},{"location":"gaussian-process-regression/#instantiation-and-data-specification","text":"A GPR object is instantiated with data specification in its constructor: GaussianProcessRegressor(const Eigen::MatrixXd& X, const Eigen::VectorXd& y, const KernelType kernel_type = KernelType::ArdMatern52, const bool use_data_normalization = true);","title":"Instantiation and Data Specification"},{"location":"gaussian-process-regression/#hyperparameter-selection","text":"Hyperparameters are set by either void SetHyperparams(const Eigen::VectorXd& kernel_hyperparams, const double noise_hyperparam); or void PerformMaximumLikelihood(const Eigen::VectorXd& kernel_hyperparams_initial, const double noise_hyperparam_initial);","title":"Hyperparameter Selection"},{"location":"gaussian-process-regression/#prediction","text":"Once a GPR object is instantiated and its hyperparameters are set, it is ready for prediction. For an unknown location \\mathbf{x} , the GPR object predicts the most likely value f by the following method: double PredictMean(const Eigen::VectorXd& x) const; It also predicts the standard deviation \\sqrt{\\text{Var}(f)} by the following method: double PredictStdev(const Eigen::VectorXd& x) const;","title":"Prediction"},{"location":"gaussian-process-regression/#useful-resources","text":"Mark Ebden. 2015. Gaussian Processes: A Quick Introduction. arXiv:1505.02965 . Carl Edward Rasmussen and Christopher K. I. Williams. 2006. Gaussian Processes for Machine Learning. The MIT Press. Online version: http://www.gaussianprocess.org/gpml/","title":"Useful Resources"},{"location":"gradient-descent/","text":"gradient-descent Gradient descent is a gradient-based local optimization method. This is probably the simplest method in this category. Header #include <mathtoolbox/gradient-descent.hpp> Internal Dependencies backtracking-line-search Math and Algorithm Line Search This implementation uses backtracking-line-search to find an appropriate step size. The initial step size needs to be specified as default_alpha . Stopping Criteria (TODO) Bound Conditions This implementation supports simple lower/upper bound conditions. Useful Resources Gradient descent - Wikipedia. https://en.wikipedia.org/wiki/Gradient_descent .","title":"gradient-descent"},{"location":"gradient-descent/#gradient-descent","text":"Gradient descent is a gradient-based local optimization method. This is probably the simplest method in this category.","title":"gradient-descent"},{"location":"gradient-descent/#header","text":"#include <mathtoolbox/gradient-descent.hpp>","title":"Header"},{"location":"gradient-descent/#internal-dependencies","text":"backtracking-line-search","title":"Internal Dependencies"},{"location":"gradient-descent/#math-and-algorithm","text":"","title":"Math and Algorithm"},{"location":"gradient-descent/#line-search","text":"This implementation uses backtracking-line-search to find an appropriate step size. The initial step size needs to be specified as default_alpha .","title":"Line Search"},{"location":"gradient-descent/#stopping-criteria","text":"(TODO)","title":"Stopping Criteria"},{"location":"gradient-descent/#bound-conditions","text":"This implementation supports simple lower/upper bound conditions.","title":"Bound Conditions"},{"location":"gradient-descent/#useful-resources","text":"Gradient descent - Wikipedia. https://en.wikipedia.org/wiki/Gradient_descent .","title":"Useful Resources"},{"location":"kernel-functions/","text":"kernel-functions Kernel functions for various techniques. Header #include <mathtoolbox/kernel-functions.hpp> Overview Automatic Relevance Determination (ARD) Squared Exponential Kernel This kernel is defined as k(\\mathbf{x}_p, \\mathbf{x}_q ; \\boldsymbol{\\theta}) = \\sigma_f^{2} \\exp \\left( - \\frac{1}{2} (\\mathbf{x}_p - \\mathbf{x}_q)^{T} \\text{diag}(\\boldsymbol{\\ell})^{-2} (\\mathbf{x}_p - \\mathbf{x}_q) \\right), where \\sigma_f^{2} (the signal variance) and \\boldsymbol{\\ell} (the characteristic length-scales) are its hyperparameters. That is, \\boldsymbol{\\theta} = \\begin{bmatrix} \\sigma_{f}^{2} \\\\ \\boldsymbol{\\ell} \\end{bmatrix} \\in \\mathbb{R}^{n + 1}_{> 0}. Automatic Relevance Determination (ARD) Matern 5/2 Kernel This kernel is defined as k(\\mathbf{x}_p, \\mathbf{x}_q ; \\boldsymbol{\\theta}) = \\sigma_f^{2} \\left( 1 + \\sqrt{5 r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell})} + \\frac{5}{3} r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell}) \\right) \\exp \\left\\{ - \\sqrt{5 r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell})} \\right\\}, where r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell}) = (\\mathbf{x}_p - \\mathbf{x}_q)^{T} \\text{diag}(\\boldsymbol{\\ell})^{-2} (\\mathbf{x}_p - \\mathbf{x}_q) and \\sigma_f^{2} (the signal variance) and \\boldsymbol{\\ell} (the characteristic length-scales) are its hyperparameters. That is, \\boldsymbol{\\theta} = \\begin{bmatrix} \\sigma_{f}^{2} \\\\ \\boldsymbol{\\ell} \\end{bmatrix} \\in \\mathbb{R}^{n + 1}_{> 0}. Useful Resources Carl Edward Rasmussen and Christopher K. I. Williams. 2006. Gaussian Processes for Machine Learning. The MIT Press. Online version: http://www.gaussianprocess.org/gpml/","title":"kernel-functions"},{"location":"kernel-functions/#kernel-functions","text":"Kernel functions for various techniques.","title":"kernel-functions"},{"location":"kernel-functions/#header","text":"#include <mathtoolbox/kernel-functions.hpp>","title":"Header"},{"location":"kernel-functions/#overview","text":"","title":"Overview"},{"location":"kernel-functions/#automatic-relevance-determination-ard-squared-exponential-kernel","text":"This kernel is defined as k(\\mathbf{x}_p, \\mathbf{x}_q ; \\boldsymbol{\\theta}) = \\sigma_f^{2} \\exp \\left( - \\frac{1}{2} (\\mathbf{x}_p - \\mathbf{x}_q)^{T} \\text{diag}(\\boldsymbol{\\ell})^{-2} (\\mathbf{x}_p - \\mathbf{x}_q) \\right), where \\sigma_f^{2} (the signal variance) and \\boldsymbol{\\ell} (the characteristic length-scales) are its hyperparameters. That is, \\boldsymbol{\\theta} = \\begin{bmatrix} \\sigma_{f}^{2} \\\\ \\boldsymbol{\\ell} \\end{bmatrix} \\in \\mathbb{R}^{n + 1}_{> 0}.","title":"Automatic Relevance Determination (ARD) Squared Exponential Kernel"},{"location":"kernel-functions/#automatic-relevance-determination-ard-matern-52-kernel","text":"This kernel is defined as k(\\mathbf{x}_p, \\mathbf{x}_q ; \\boldsymbol{\\theta}) = \\sigma_f^{2} \\left( 1 + \\sqrt{5 r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell})} + \\frac{5}{3} r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell}) \\right) \\exp \\left\\{ - \\sqrt{5 r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell})} \\right\\}, where r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell}) = (\\mathbf{x}_p - \\mathbf{x}_q)^{T} \\text{diag}(\\boldsymbol{\\ell})^{-2} (\\mathbf{x}_p - \\mathbf{x}_q) and \\sigma_f^{2} (the signal variance) and \\boldsymbol{\\ell} (the characteristic length-scales) are its hyperparameters. That is, \\boldsymbol{\\theta} = \\begin{bmatrix} \\sigma_{f}^{2} \\\\ \\boldsymbol{\\ell} \\end{bmatrix} \\in \\mathbb{R}^{n + 1}_{> 0}.","title":"Automatic Relevance Determination (ARD) Matern 5/2 Kernel"},{"location":"kernel-functions/#useful-resources","text":"Carl Edward Rasmussen and Christopher K. I. Williams. 2006. Gaussian Processes for Machine Learning. The MIT Press. Online version: http://www.gaussianprocess.org/gpml/","title":"Useful Resources"},{"location":"l-bfgs/","text":"l-bfgs The Limited-memory BFGS method (L-BFGS) is a numerical optimization algorithm that is one of the most popular choices among quasi-Newton methods. Header #include <mathtoolbox/l-bfgs.hpp> Internal Dependencies strong-wolfe-conditions-line-search Math and Algorithm We follow Nocedal and Wright (2006) (Chapter 7). Inverse Hessian Initialization This implementation adopts the strategy described in Equation 7.20: \\mathbf{H}_k^0 \\leftarrow \\frac{\\mathbf{y}_{k - 1}^T \\mathbf{s}_{k - 1}}{\\mathbf{y}_{k - 1}^T \\mathbf{y}_{k - 1}} \\mathbf{I}. See the book for details. Line Search This implementation uses strong-wolfe-conditions-line-search to find an appropriate step size. Useful Resources Jorge Nocedal and Stephen J. Wright. 2006. Numerical optimization (2nd ed.). Springer. DOI: https://doi.org/10.1007/978-0-387-40065-5","title":"l-bfgs"},{"location":"l-bfgs/#l-bfgs","text":"The Limited-memory BFGS method (L-BFGS) is a numerical optimization algorithm that is one of the most popular choices among quasi-Newton methods.","title":"l-bfgs"},{"location":"l-bfgs/#header","text":"#include <mathtoolbox/l-bfgs.hpp>","title":"Header"},{"location":"l-bfgs/#internal-dependencies","text":"strong-wolfe-conditions-line-search","title":"Internal Dependencies"},{"location":"l-bfgs/#math-and-algorithm","text":"We follow Nocedal and Wright (2006) (Chapter 7).","title":"Math and Algorithm"},{"location":"l-bfgs/#inverse-hessian-initialization","text":"This implementation adopts the strategy described in Equation 7.20: \\mathbf{H}_k^0 \\leftarrow \\frac{\\mathbf{y}_{k - 1}^T \\mathbf{s}_{k - 1}}{\\mathbf{y}_{k - 1}^T \\mathbf{y}_{k - 1}} \\mathbf{I}. See the book for details.","title":"Inverse Hessian Initialization"},{"location":"l-bfgs/#line-search","text":"This implementation uses strong-wolfe-conditions-line-search to find an appropriate step size.","title":"Line Search"},{"location":"l-bfgs/#useful-resources","text":"Jorge Nocedal and Stephen J. Wright. 2006. Numerical optimization (2nd ed.). Springer. DOI: https://doi.org/10.1007/978-0-387-40065-5","title":"Useful Resources"},{"location":"log-determinant/","text":"log-determinant Techniques for calculating log-determinant of a matrix. Header #include <mathtoolbox/log-determinant.hpp> Log-Determinant of a Symmetric Positive-Definite Matrix Let \\mathbf{K} \\in \\mathbb{R}^{n \\times n} be a symmetric positive-definite matrix such a covariance matrix . The goal is to calculate the log of its determinant: \\log(\\det(\\mathbf{K})). This calculation often appears when handling a log-likelihood of some Gaussian-related event. A naive way is to calculate the determinant explicitly and then calculate its log. However, this way is known for its numerical instability (i.e., likely to go to negative infinity). This module offers a function to calculate log-determinant much more stably. Useful Resources Compute the log-determinant of a matrix - The DO Loop. https://blogs.sas.com/content/iml/2012/10/31/compute-the-log-determinant-of-a-matrix.html .","title":"log-determinant"},{"location":"log-determinant/#log-determinant","text":"Techniques for calculating log-determinant of a matrix.","title":"log-determinant"},{"location":"log-determinant/#header","text":"#include <mathtoolbox/log-determinant.hpp>","title":"Header"},{"location":"log-determinant/#log-determinant-of-a-symmetric-positive-definite-matrix","text":"Let \\mathbf{K} \\in \\mathbb{R}^{n \\times n} be a symmetric positive-definite matrix such a covariance matrix . The goal is to calculate the log of its determinant: \\log(\\det(\\mathbf{K})). This calculation often appears when handling a log-likelihood of some Gaussian-related event. A naive way is to calculate the determinant explicitly and then calculate its log. However, this way is known for its numerical instability (i.e., likely to go to negative infinity). This module offers a function to calculate log-determinant much more stably.","title":"Log-Determinant of a Symmetric Positive-Definite Matrix"},{"location":"log-determinant/#useful-resources","text":"Compute the log-determinant of a matrix - The DO Loop. https://blogs.sas.com/content/iml/2012/10/31/compute-the-log-determinant-of-a-matrix.html .","title":"Useful Resources"},{"location":"matrix-inversion/","text":"matrix-inversion Matrix inversion techniques. Header #include <mathtoolbox/matrix-inversion.hpp> Block Matrix Inversion About Inverse of a block (partitioned) matrix \\mathbf{X} = \\begin{bmatrix} \\mathbf{A} & \\mathbf{B} \\\\ \\mathbf{C} & \\mathbf{D} \\end{bmatrix}, where \\mathbf{A} and \\mathbf{D} are square matrices, can be calculated as \\begin{bmatrix} \\mathbf{A} & \\mathbf{B} \\\\ \\mathbf{C} & \\mathbf{D} \\end{bmatrix}^{-1} = \\begin{bmatrix} \\mathbf{A}^{-1} ( \\mathbf{I} + \\mathbf{B} ( \\mathbf{D} - \\mathbf{C} \\mathbf{A}^{-1} \\mathbf{B} )^{-1} \\mathbf{C} \\mathbf{A}^{-1} ) & - \\mathbf{A}^{-1} \\mathbf{B} ( \\mathbf{D} - \\mathbf{C} \\mathbf{A}^{-1} \\mathbf{B} )^{-1} \\\\ - ( \\mathbf{D} - \\mathbf{C} \\mathbf{A}^{-1} \\mathbf{B} )^{-1} \\mathbf{C} \\mathbf{A}^{-1} & ( \\mathbf{D} - \\mathbf{C} \\mathbf{A}^{-1} \\mathbf{B} )^{-1} \\end{bmatrix} This technique is useful particularly when \\mathbf{A} is relatively large (compared to \\mathbf{D} ) and \\mathbf{A}^{-1} is known. API This module provides the following function: Eigen::MatrixXd GetInverseUsingUpperLeftBlockInverse(const Eigen::MatrixXd& matrix, const Eigen::MatrixXd& upper_left_block_inverse); where upper_left_block_inverse corresponds to \\mathbf{A}^{-1} . Performance (Casual Comparison) When \\mathbf{X} was a random matrix, and the size of \\mathbf{X} was 3,000 and that of \\mathbf{A} was 2,999, a naive approach (i.e., the LU decomposition from Eigen) took 5847 milliseconds to obtain \\mathbf{X}^{-1} while the block inversion approach took only 121 milliseconds. Useful Resources Block matrix - Wikipedia. https://en.wikipedia.org/wiki/Block_matrix .","title":"matrix-inversion"},{"location":"matrix-inversion/#matrix-inversion","text":"Matrix inversion techniques.","title":"matrix-inversion"},{"location":"matrix-inversion/#header","text":"#include <mathtoolbox/matrix-inversion.hpp>","title":"Header"},{"location":"matrix-inversion/#block-matrix-inversion","text":"","title":"Block Matrix Inversion"},{"location":"matrix-inversion/#about","text":"Inverse of a block (partitioned) matrix \\mathbf{X} = \\begin{bmatrix} \\mathbf{A} & \\mathbf{B} \\\\ \\mathbf{C} & \\mathbf{D} \\end{bmatrix}, where \\mathbf{A} and \\mathbf{D} are square matrices, can be calculated as \\begin{bmatrix} \\mathbf{A} & \\mathbf{B} \\\\ \\mathbf{C} & \\mathbf{D} \\end{bmatrix}^{-1} = \\begin{bmatrix} \\mathbf{A}^{-1} ( \\mathbf{I} + \\mathbf{B} ( \\mathbf{D} - \\mathbf{C} \\mathbf{A}^{-1} \\mathbf{B} )^{-1} \\mathbf{C} \\mathbf{A}^{-1} ) & - \\mathbf{A}^{-1} \\mathbf{B} ( \\mathbf{D} - \\mathbf{C} \\mathbf{A}^{-1} \\mathbf{B} )^{-1} \\\\ - ( \\mathbf{D} - \\mathbf{C} \\mathbf{A}^{-1} \\mathbf{B} )^{-1} \\mathbf{C} \\mathbf{A}^{-1} & ( \\mathbf{D} - \\mathbf{C} \\mathbf{A}^{-1} \\mathbf{B} )^{-1} \\end{bmatrix} This technique is useful particularly when \\mathbf{A} is relatively large (compared to \\mathbf{D} ) and \\mathbf{A}^{-1} is known.","title":"About"},{"location":"matrix-inversion/#api","text":"This module provides the following function: Eigen::MatrixXd GetInverseUsingUpperLeftBlockInverse(const Eigen::MatrixXd& matrix, const Eigen::MatrixXd& upper_left_block_inverse); where upper_left_block_inverse corresponds to \\mathbf{A}^{-1} .","title":"API"},{"location":"matrix-inversion/#performance-casual-comparison","text":"When \\mathbf{X} was a random matrix, and the size of \\mathbf{X} was 3,000 and that of \\mathbf{A} was 2,999, a naive approach (i.e., the LU decomposition from Eigen) took 5847 milliseconds to obtain \\mathbf{X}^{-1} while the block inversion approach took only 121 milliseconds.","title":"Performance (Casual Comparison)"},{"location":"matrix-inversion/#useful-resources","text":"Block matrix - Wikipedia. https://en.wikipedia.org/wiki/Block_matrix .","title":"Useful Resources"},{"location":"probability-distributions/","text":"probability-distributions Probability distributions for statistical estimation. Header #include <mathtoolbox/probability-distributions.hpp> Overview The following probability distributions and their first derivatives are supported: Standard normal distribution: \\mathcal{N}(x \\mid 0, 1) Normal distribution: \\mathcal{N}(x \\mid \\mu, \\sigma^{2}) Log-normal distribution: \\mathcal{LN}(x \\mid \\mu, \\sigma^{2}) In statistical estimation, taking logarithms of probabilities is often necessary. For this purpose, the following probability distributions and their derivatives are supported: Log of log-normal distribution: \\log \\{ \\mathcal{LN}(x \\mid \\mu, \\sigma^{2}) \\} Useful Resources Normal distribution - Wikipedia. https://en.wikipedia.org/wiki/Normal_distribution . Log-normal distribution - Wikipedia. https://en.wikipedia.org/wiki/Log-normal_distribution .","title":"probability-distributions"},{"location":"probability-distributions/#probability-distributions","text":"Probability distributions for statistical estimation.","title":"probability-distributions"},{"location":"probability-distributions/#header","text":"#include <mathtoolbox/probability-distributions.hpp>","title":"Header"},{"location":"probability-distributions/#overview","text":"The following probability distributions and their first derivatives are supported: Standard normal distribution: \\mathcal{N}(x \\mid 0, 1) Normal distribution: \\mathcal{N}(x \\mid \\mu, \\sigma^{2}) Log-normal distribution: \\mathcal{LN}(x \\mid \\mu, \\sigma^{2}) In statistical estimation, taking logarithms of probabilities is often necessary. For this purpose, the following probability distributions and their derivatives are supported: Log of log-normal distribution: \\log \\{ \\mathcal{LN}(x \\mid \\mu, \\sigma^{2}) \\}","title":"Overview"},{"location":"probability-distributions/#useful-resources","text":"Normal distribution - Wikipedia. https://en.wikipedia.org/wiki/Normal_distribution . Log-normal distribution - Wikipedia. https://en.wikipedia.org/wiki/Log-normal_distribution .","title":"Useful Resources"},{"location":"rbf-interpolation/","text":"rbf-interpolation Radial basis function (RBF) network for scattered data interpolation and function approximation. Header #include <mathtoolbox/rbf-interpolation.hpp> Math Overview Given input data: \\{ (\\mathbf{x}_i, y_i) \\}_{i = 1, \\ldots, n}, this technique calculates an interpolated value y \\in \\mathbb{R} for a specified point \\mathbf{x} \\in \\mathbb{R}^{m} by y = f(\\mathbf{x}) = \\sum_{i = 1}^{n} w_{i} \\phi( \\| \\mathbf{x} - \\mathbf{x}_{i} \\|), where \\phi : \\mathbb{R}_{> 0} \\rightarrow \\mathbb{R} is a user-selected RBF, and \\mathbf{w} = \\begin{bmatrix} w_1 & \\cdots & w_n \\end{bmatrix}^T \\in \\mathbb{R}^{n} are the weights that are calculated in pre-computation. Pre-Computation The weight values need to be calculated in pre-computation. Let \\mathbf{\\Phi} = \\begin{bmatrix} \\phi_{1, 1} & \\cdots & \\phi_{1, n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\phi_{n, 1} & \\cdots & \\phi_{n, n} \\end{bmatrix} \\in \\mathbb{R}^{n \\times n}, where \\phi_{i, j} = \\phi(\\| \\mathbf{x}_i - \\mathbf{x}_j \\|). The following linear system is solved for \\mathbf{w} : \\mathbf{\\Phi} \\mathbf{w} = \\mathbf{y}. LU decomposition can be used for solving this problem. This approach offers exact interpolation: the interpolated function passes through all the scattered data points exactly. Pre-Computation with Regularization The original formulation above is not robust when the data points are dense and noisy. For such cases, it is effective to use a feature called regularization in pre-computation. In other words, this feature enables scattered data approximation rather than scattered data (exact) interpolation . This feature is achieved by considering a regularization term in the calculation of the weight values. More specifically, the following minimization problem is solved: \\min_{\\mathbf{w}} \\left\\{ \\| \\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\| \\mathbf{w} \\|^2 \\right\\}. The derivative of this objective function with respect to \\mathbf{w} is \\begin{eqnarray*} && \\frac{\\partial}{\\partial \\mathbf{w}} \\left\\{ \\| \\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\| \\mathbf{w} \\|^2 \\right\\} \\\\ &=& \\frac{\\partial}{\\partial \\mathbf{w}} \\| \\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\frac{\\partial}{\\partial \\mathbf{w}} \\| \\mathbf{w} \\|^2 \\\\ &=& 2 \\mathbf{\\Phi}^T (\\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y}) + 2 \\lambda \\mathbf{w} \\\\ &=& 2 \\left\\{ (\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I}) \\mathbf{w} - \\mathbf{\\Phi}^T \\mathbf{y} \\right\\}. \\end{eqnarray*} Thus, the solution of the above minimization problem is obtained by solving the below linear system: (\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I}) \\mathbf{w} = \\mathbf{\\Phi}^T \\mathbf{y}. Adding Polynomial Term (and Polyharmonic Spline) The above techniques can be extended by adding a polynomial term; that is, y = f(\\mathbf{x}) = \\sum_{i = 1}^{n} w_{i} \\phi( \\| \\mathbf{x} - \\mathbf{x}_{i} \\|) + v_{0} + v_{1} x_{1} + \\cdots + v_{m} x_{m}. This extension is also referred to as polyharmonic spline when it is used with polyharmonic RBF kernels (e.g., the linear kernel, the thin plate spline kernel, and the cubic kernel). This extension offers several nice properties; for example, this makes the extrapolation behavior much more reasonable as shown below. See [Anjyo et al. 2014] for details. This extension is recommended to always use and enabled by default. Usage First, instantiate the class RbfInterpolator . In its constructor, an arbitrary RBF kernel (in the form of std::function<double(double)> ) can be specified. The followings are pre-implemented as function objects and can be easily specified: GaussianRbfKernel : \\phi(x) = \\exp(- \\epsilon x^{2}) LinearRbfKernel : \\phi(x) = x ThinPlateSplineRbfKernel : \\phi(x) = x^{2} \\log(x) CubicRbfKernel : \\phi(x) = x^{3} If no kernel is passed to the constructor, ThinPlateSplineRbfKernel is chosen by default. Then, set the target scattered data by the method: void SetData(const Eigen::MatrixXd& X, const Eigen::VectorXd& y); where \\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_{1} & \\cdots & \\mathbf{x}_{n} \\end{bmatrix} \\in \\mathbb{R}^{m \\times n} represents the data points and \\mathbf{y} = \\begin{bmatrix} y_1 & \\cdots & y_n \\end{bmatrix}^T \\in \\mathbb{R}^{m} represents their values. Next, calculate the weight values by the method: void ComputeWeights(const bool use_regularization = false, const double lambda = 0.001); When use_regularization is set true , the weights are calculated in the manner of scattered data approximation , rather than scattered data interpolation. When the data is noisy, approximation is usually a better choice. Once the above procedures are performed, the instance is ready to calculate interpolated values. This is performed by the method double CalcValue(const Eigen::VectorXd& x) const; Time Complexity The pre-computation needs to solve a linear system, which takes more than O(n^{2}) . An interpolated value calculation takes O(n) . See [Carr et al. 2001] for details. Useful Resources Ken Anjyo, J. P. Lewis, and Fr\u00e9d\u00e9ric Pighin. 2014. Scattered data interpolation for computer graphics. In ACM SIGGRAPH 2014 Courses (SIGGRAPH '14). Article 27, 69 pages. DOI: https://doi.org/10.1145/2614028.2615425 J. C. Carr, R. K. Beatson, J. B. Cherrie, T. J. Mitchell, W. R. Fright, B. C. McCallum, and T. R. Evans. 2001. Reconstruction and representation of 3D objects with radial basis functions. In Proc. SIGGRAPH '01. 67\u201376. DOI: https://doi.org/10.1145/383259.383266 Polyharmonic spline. https://en.wikipedia.org/wiki/Polyharmonic_spline","title":"rbf-interpolation"},{"location":"rbf-interpolation/#rbf-interpolation","text":"Radial basis function (RBF) network for scattered data interpolation and function approximation.","title":"rbf-interpolation"},{"location":"rbf-interpolation/#header","text":"#include <mathtoolbox/rbf-interpolation.hpp>","title":"Header"},{"location":"rbf-interpolation/#math","text":"","title":"Math"},{"location":"rbf-interpolation/#overview","text":"Given input data: \\{ (\\mathbf{x}_i, y_i) \\}_{i = 1, \\ldots, n}, this technique calculates an interpolated value y \\in \\mathbb{R} for a specified point \\mathbf{x} \\in \\mathbb{R}^{m} by y = f(\\mathbf{x}) = \\sum_{i = 1}^{n} w_{i} \\phi( \\| \\mathbf{x} - \\mathbf{x}_{i} \\|), where \\phi : \\mathbb{R}_{> 0} \\rightarrow \\mathbb{R} is a user-selected RBF, and \\mathbf{w} = \\begin{bmatrix} w_1 & \\cdots & w_n \\end{bmatrix}^T \\in \\mathbb{R}^{n} are the weights that are calculated in pre-computation.","title":"Overview"},{"location":"rbf-interpolation/#pre-computation","text":"The weight values need to be calculated in pre-computation. Let \\mathbf{\\Phi} = \\begin{bmatrix} \\phi_{1, 1} & \\cdots & \\phi_{1, n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\phi_{n, 1} & \\cdots & \\phi_{n, n} \\end{bmatrix} \\in \\mathbb{R}^{n \\times n}, where \\phi_{i, j} = \\phi(\\| \\mathbf{x}_i - \\mathbf{x}_j \\|). The following linear system is solved for \\mathbf{w} : \\mathbf{\\Phi} \\mathbf{w} = \\mathbf{y}. LU decomposition can be used for solving this problem. This approach offers exact interpolation: the interpolated function passes through all the scattered data points exactly.","title":"Pre-Computation"},{"location":"rbf-interpolation/#pre-computation-with-regularization","text":"The original formulation above is not robust when the data points are dense and noisy. For such cases, it is effective to use a feature called regularization in pre-computation. In other words, this feature enables scattered data approximation rather than scattered data (exact) interpolation . This feature is achieved by considering a regularization term in the calculation of the weight values. More specifically, the following minimization problem is solved: \\min_{\\mathbf{w}} \\left\\{ \\| \\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\| \\mathbf{w} \\|^2 \\right\\}. The derivative of this objective function with respect to \\mathbf{w} is \\begin{eqnarray*} && \\frac{\\partial}{\\partial \\mathbf{w}} \\left\\{ \\| \\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\| \\mathbf{w} \\|^2 \\right\\} \\\\ &=& \\frac{\\partial}{\\partial \\mathbf{w}} \\| \\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\frac{\\partial}{\\partial \\mathbf{w}} \\| \\mathbf{w} \\|^2 \\\\ &=& 2 \\mathbf{\\Phi}^T (\\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y}) + 2 \\lambda \\mathbf{w} \\\\ &=& 2 \\left\\{ (\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I}) \\mathbf{w} - \\mathbf{\\Phi}^T \\mathbf{y} \\right\\}. \\end{eqnarray*} Thus, the solution of the above minimization problem is obtained by solving the below linear system: (\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I}) \\mathbf{w} = \\mathbf{\\Phi}^T \\mathbf{y}.","title":"Pre-Computation with Regularization"},{"location":"rbf-interpolation/#adding-polynomial-term-and-polyharmonic-spline","text":"The above techniques can be extended by adding a polynomial term; that is, y = f(\\mathbf{x}) = \\sum_{i = 1}^{n} w_{i} \\phi( \\| \\mathbf{x} - \\mathbf{x}_{i} \\|) + v_{0} + v_{1} x_{1} + \\cdots + v_{m} x_{m}. This extension is also referred to as polyharmonic spline when it is used with polyharmonic RBF kernels (e.g., the linear kernel, the thin plate spline kernel, and the cubic kernel). This extension offers several nice properties; for example, this makes the extrapolation behavior much more reasonable as shown below. See [Anjyo et al. 2014] for details. This extension is recommended to always use and enabled by default.","title":"Adding Polynomial Term (and Polyharmonic Spline)"},{"location":"rbf-interpolation/#usage","text":"First, instantiate the class RbfInterpolator . In its constructor, an arbitrary RBF kernel (in the form of std::function<double(double)> ) can be specified. The followings are pre-implemented as function objects and can be easily specified: GaussianRbfKernel : \\phi(x) = \\exp(- \\epsilon x^{2}) LinearRbfKernel : \\phi(x) = x ThinPlateSplineRbfKernel : \\phi(x) = x^{2} \\log(x) CubicRbfKernel : \\phi(x) = x^{3} If no kernel is passed to the constructor, ThinPlateSplineRbfKernel is chosen by default. Then, set the target scattered data by the method: void SetData(const Eigen::MatrixXd& X, const Eigen::VectorXd& y); where \\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_{1} & \\cdots & \\mathbf{x}_{n} \\end{bmatrix} \\in \\mathbb{R}^{m \\times n} represents the data points and \\mathbf{y} = \\begin{bmatrix} y_1 & \\cdots & y_n \\end{bmatrix}^T \\in \\mathbb{R}^{m} represents their values. Next, calculate the weight values by the method: void ComputeWeights(const bool use_regularization = false, const double lambda = 0.001); When use_regularization is set true , the weights are calculated in the manner of scattered data approximation , rather than scattered data interpolation. When the data is noisy, approximation is usually a better choice. Once the above procedures are performed, the instance is ready to calculate interpolated values. This is performed by the method double CalcValue(const Eigen::VectorXd& x) const;","title":"Usage"},{"location":"rbf-interpolation/#time-complexity","text":"The pre-computation needs to solve a linear system, which takes more than O(n^{2}) . An interpolated value calculation takes O(n) . See [Carr et al. 2001] for details.","title":"Time Complexity"},{"location":"rbf-interpolation/#useful-resources","text":"Ken Anjyo, J. P. Lewis, and Fr\u00e9d\u00e9ric Pighin. 2014. Scattered data interpolation for computer graphics. In ACM SIGGRAPH 2014 Courses (SIGGRAPH '14). Article 27, 69 pages. DOI: https://doi.org/10.1145/2614028.2615425 J. C. Carr, R. K. Beatson, J. B. Cherrie, T. J. Mitchell, W. R. Fright, B. C. McCallum, and T. R. Evans. 2001. Reconstruction and representation of 3D objects with radial basis functions. In Proc. SIGGRAPH '01. 67\u201376. DOI: https://doi.org/10.1145/383259.383266 Polyharmonic spline. https://en.wikipedia.org/wiki/Polyharmonic_spline","title":"Useful Resources"},{"location":"som/","text":"som Self-organizing map (SOM) for dimensionality reduction and low-dimensional embedding. This technique is useful for understanding high-dimensional data via visualization. Header #include <mathtoolbox/som.hpp> Internal Dependencies data-normalization Math Terms In this library, the following terms are used: Data space: The space that the data points exist. It is typically high-dimensional. Latent space: The space that the map is constructed. It is typically (and in this library restricted to) one- or two-dimensional. Node: The element of the map. It has fixed coordinates in the latent space and also has coordinates in the data space that will be learned by the SOM algorithm. Update This library implements a batch-style SOM algorithm rather than online-style ones. That is, for each update step, all the data points are handled equally and contribute to the update of the node coordinates simultaneously. Let \\mathbf{X} \\in \\mathbb{R}^{D \\times N} be the data matrix (each column represents a data point) and \\mathbf{Y} \\in \\mathbb{R}^{D \\times K} be the map matrix (each column represents coordinates of a node). The batch-style update is written as \\mathbf{Y} = ( \\mathbf{G}^{-1} \\mathbf{H} \\mathbf{B} \\mathbf{X}^{T} )^{T}. See the source code for the definitions of \\mathbf{G} , \\mathbf{H} , and \\mathbf{B} . Neighborhood Function This library uses a Gaussian function with a decreasing variance: \\sigma^{2}(t) = \\max \\left[ \\sigma^{2}_{0} \\exp \\left(- \\frac{t}{s} \\right), \\sigma^{2}_{\\min} \\right], where t is the iteration count, s is a user-specified parameter for controlling the speed of decrease, and \\sigma^{2}_{0} and \\sigma^{2}_{\\min} are user-specified initial and minimum variances, respectively. Data Normalization This library offers an option to perform data normalization, which is recommended to use in general. Examples The following is an example of applying the algorithm to the pixel RGB values of a target image and learning its 2D color manifold. The above map was generated through 30 iterations. The learning process is visualized as below. (Your browser doesn't support video playing.) Self-organizing map is often assumed to be two-dimensional, but it is also possible to use other dimensionalities for the latent space. The below is an example of learning a one-dimensional map. Useful Resources Chuong H. Nguyen, Tobias Ritschel, and Hans-Peter Seidel. 2015. Data-Driven Color Manifolds. ACM Trans. Graph. 34, 2, 20:1--20:9 (March 2015). DOI: https://doi.org/10.1145/2699645","title":"som"},{"location":"som/#som","text":"Self-organizing map (SOM) for dimensionality reduction and low-dimensional embedding. This technique is useful for understanding high-dimensional data via visualization.","title":"som"},{"location":"som/#header","text":"#include <mathtoolbox/som.hpp>","title":"Header"},{"location":"som/#internal-dependencies","text":"data-normalization","title":"Internal Dependencies"},{"location":"som/#math","text":"","title":"Math"},{"location":"som/#terms","text":"In this library, the following terms are used: Data space: The space that the data points exist. It is typically high-dimensional. Latent space: The space that the map is constructed. It is typically (and in this library restricted to) one- or two-dimensional. Node: The element of the map. It has fixed coordinates in the latent space and also has coordinates in the data space that will be learned by the SOM algorithm.","title":"Terms"},{"location":"som/#update","text":"This library implements a batch-style SOM algorithm rather than online-style ones. That is, for each update step, all the data points are handled equally and contribute to the update of the node coordinates simultaneously. Let \\mathbf{X} \\in \\mathbb{R}^{D \\times N} be the data matrix (each column represents a data point) and \\mathbf{Y} \\in \\mathbb{R}^{D \\times K} be the map matrix (each column represents coordinates of a node). The batch-style update is written as \\mathbf{Y} = ( \\mathbf{G}^{-1} \\mathbf{H} \\mathbf{B} \\mathbf{X}^{T} )^{T}. See the source code for the definitions of \\mathbf{G} , \\mathbf{H} , and \\mathbf{B} .","title":"Update"},{"location":"som/#neighborhood-function","text":"This library uses a Gaussian function with a decreasing variance: \\sigma^{2}(t) = \\max \\left[ \\sigma^{2}_{0} \\exp \\left(- \\frac{t}{s} \\right), \\sigma^{2}_{\\min} \\right], where t is the iteration count, s is a user-specified parameter for controlling the speed of decrease, and \\sigma^{2}_{0} and \\sigma^{2}_{\\min} are user-specified initial and minimum variances, respectively.","title":"Neighborhood Function"},{"location":"som/#data-normalization","text":"This library offers an option to perform data normalization, which is recommended to use in general.","title":"Data Normalization"},{"location":"som/#examples","text":"The following is an example of applying the algorithm to the pixel RGB values of a target image and learning its 2D color manifold. The above map was generated through 30 iterations. The learning process is visualized as below. (Your browser doesn't support video playing.) Self-organizing map is often assumed to be two-dimensional, but it is also possible to use other dimensionalities for the latent space. The below is an example of learning a one-dimensional map.","title":"Examples"},{"location":"som/#useful-resources","text":"Chuong H. Nguyen, Tobias Ritschel, and Hans-Peter Seidel. 2015. Data-Driven Color Manifolds. ACM Trans. Graph. 34, 2, 20:1--20:9 (March 2015). DOI: https://doi.org/10.1145/2699645","title":"Useful Resources"},{"location":"strong-wolfe-conditions-line-search/","text":"strong-wolfe-conditions-line-search A line search method for finding a step size that satisfies the strong Wolfe conditions (i.e., the Armijo (i.e., sufficient decrease) condition and the curvature condition). Header #include <mathtoolbox/strong-wolfe-conditions-line-search.hpp> Math and Algorithm We follow Nocedal and Wright (2006) (Chapter 3, specifically Algorithm 3.5). Useful Resources Jorge Nocedal and Stephen J. Wright. 2006. Numerical optimization (2nd ed.). Springer. DOI: https://doi.org/10.1007/978-0-387-40065-5","title":"strong-wolfe-conditions-line-search"},{"location":"strong-wolfe-conditions-line-search/#strong-wolfe-conditions-line-search","text":"A line search method for finding a step size that satisfies the strong Wolfe conditions (i.e., the Armijo (i.e., sufficient decrease) condition and the curvature condition).","title":"strong-wolfe-conditions-line-search"},{"location":"strong-wolfe-conditions-line-search/#header","text":"#include <mathtoolbox/strong-wolfe-conditions-line-search.hpp>","title":"Header"},{"location":"strong-wolfe-conditions-line-search/#math-and-algorithm","text":"We follow Nocedal and Wright (2006) (Chapter 3, specifically Algorithm 3.5).","title":"Math and Algorithm"},{"location":"strong-wolfe-conditions-line-search/#useful-resources","text":"Jorge Nocedal and Stephen J. Wright. 2006. Numerical optimization (2nd ed.). Springer. DOI: https://doi.org/10.1007/978-0-387-40065-5","title":"Useful Resources"}]}